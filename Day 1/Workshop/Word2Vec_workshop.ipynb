{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_workshop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_89yP2qzEui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install plot_keras_history\n",
        "!pip install seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeiZ_Cqvzoqe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "877b2395-f48a-46a3-a072-89a0be197799"
      },
      "source": [
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda, Reshape\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import dot\n",
        "from tensorflow.keras.activations import relu\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "import gensim\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZazoZFH0BZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8948bc34-1131-4b02-f5bf-7fb885737a2f"
      },
      "source": [
        "# using nltk tokenizer.  \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co69rL7jzrh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "2dc66ad6-2401-4fd2-8745-6e726837982c"
      },
      "source": [
        "#Data Preparation \n",
        "\n",
        "AlotOftext = \"\"\"Language users never choose words randomly, and language is essentially\n",
        "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
        "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
        "the null hypothesis will never be true. Moreover, where there is enough\n",
        "data, we shall (almost) always be able to establish that it is not true. In\n",
        "corpus studies, we frequently do have enough data, so the fact that a relation \n",
        "between two phenomena is demonstrably non-random, does not support the inference \n",
        "that it is not arbitrary. We present experimental evidence\n",
        "of how arbitrary associations between word frequencies and corpora are\n",
        "systematically non-random. We review literature in which hypothesis testing \n",
        "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
        "\n",
        "\n",
        "\n",
        "#Tokenize text\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(AlotOftext)]\n",
        "\n",
        "#Create Vocab as a Dictionary\n",
        "vocab = Dictionary(tokenized_text)\n",
        "print(dict(vocab.items()))\n",
        "\n",
        "print(vocab.token2id['corpora'])\n",
        "print(vocab[2])\n",
        "sent0 = tokenized_text[0]\n",
        "print(vocab.doc2idx(sent0))\n",
        "\n",
        "vocab.add_documents([['PAD']])\n",
        "dict(vocab.items())\n",
        "print(vocab.token2id['PAD'])\n",
        "\n",
        "corpusByWordID = list()\n",
        "for sent in  tokenized_text:\n",
        "    corpusByWordID.append(vocab.doc2idx(sent))\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 100\n",
        "window_size = 2 # context window size\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(vocab.items())[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: ',', 1: '.', 2: 'and', 3: 'choose', 4: 'essentially', 5: 'is', 6: 'language', 7: 'never', 8: 'non-random', 9: 'randomly', 10: 'users', 11: 'words', 12: 'a', 13: 'hypothesis', 14: 'null', 15: 'posits', 16: 'randomness', 17: 'statistical', 18: 'testing', 19: 'uses', 20: 'which', 21: 'at', 22: 'be', 23: 'corpora', 24: 'hence', 25: 'in', 26: 'linguistic', 27: 'look', 28: 'phenomena', 29: 'the', 30: 'true', 31: 'we', 32: 'when', 33: 'will', 34: '(', 35: ')', 36: 'able', 37: 'almost', 38: 'always', 39: 'data', 40: 'enough', 41: 'establish', 42: 'it', 43: 'moreover', 44: 'not', 45: 'shall', 46: 'that', 47: 'there', 48: 'to', 49: 'where', 50: 'arbitrary', 51: 'between', 52: 'corpus', 53: 'demonstrably', 54: 'do', 55: 'does', 56: 'fact', 57: 'frequently', 58: 'have', 59: 'inference', 60: 'relation', 61: 'so', 62: 'studies', 63: 'support', 64: 'two', 65: 'are', 66: 'associations', 67: 'evidence', 68: 'experimental', 69: 'frequencies', 70: 'how', 71: 'of', 72: 'present', 73: 'systematically', 74: 'word', 75: 'been', 76: 'has', 77: 'led', 78: 'literature', 79: 'misleading', 80: 'often', 81: 'or', 82: 'results', 83: 'review', 84: 'show', 85: 'unhelpful', 86: 'used'}\n",
            "23\n",
            "and\n",
            "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]\n",
            "87\n",
            "Vocabulary Size: 88\n",
            "Vocabulary Sample: [(0, ','), (1, '.'), (2, 'and'), (3, 'choose'), (4, 'essentially'), (5, 'is'), (6, 'language'), (7, 'never'), (8, 'non-random'), (9, 'randomly')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBxqCqAL1BCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create CBOW Training data\n",
        "def generate_cbow_context_word_pairs(corpusByID, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for sent in corpusByID:\n",
        "        sentence_length = len(sent)\n",
        "        for index, word in enumerate(sent):\n",
        "            context_words = []\n",
        "            label_word   = []            \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            \n",
        "            context_words.append([sent[i] \n",
        "                                 for i in range(start, end) \n",
        "                                 if 0 <= i < sentence_length \n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "            if start<0:\n",
        "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='pre',value=vocab.token2id['PAD'])\n",
        "                y = np_utils.to_categorical(label_word, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "                continue\n",
        "            if end>=sentence_length:\n",
        "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='post',value=vocab.token2id['PAD'])\n",
        "                y = np_utils.to_categorical(label_word, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "                continue\n",
        "            else:\n",
        "                X.append(sequence.pad_sequences(context_words, maxlen=context_length))\n",
        "                Y.append(y)\n",
        "                continue\n",
        "           \n",
        "    return X,Y\n",
        "            \n",
        "# Test this out for some samples\n",
        "\n",
        "\n",
        "X,Y = generate_cbow_context_word_pairs(corpusByWordID, window_size, vocab_size) \n",
        "   \n",
        "for x, y in zip(X,Y):\n",
        "    print('Context (X):', [vocab[w] for w in x[0]], '-> Target (Y):', vocab[np.argwhere(y[0])[0][0]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH_iy4Lh1JUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the model\n",
        "cbow = Sequential()\n",
        "###hint:output_dim = the shape of embedding matrix\n",
        "###hint:input_length = the length of training sample\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=???, input_length=???))\n",
        "cbow.add(Lambda(lambda x: relu(K.mean(x, axis=1)), output_shape=(embed_size,)))\n",
        "###hint:the total numbser of possible labels/words\n",
        "###hint:activation='softmax' or 'sigmoid'\n",
        "cbow.add(Dense(???, activation='???'))\n",
        "###hint:loss='categorical_crossentropy' or 'binary_crossentropy'\n",
        "cbow.compile(loss='???', optimizer='sgd')\n",
        "cbow.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuVqi7WGDxNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train the model\n",
        "\n",
        "for epoch in range(1000):\n",
        "    loss = 0.\n",
        "    for x, y in zip(X,Y):\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "    print(epoch, loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxPVT_G-1RYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save the wordvectors\n",
        "f = open('Cbow_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = cbow.get_weights()[0]\n",
        "for key in vocab:\n",
        "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
        "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1H3zTpE1Uwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load the vectors back and validate\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
        "\n",
        "w2v.most_similar(positive=['that'])\n",
        "w2v.most_similar(negative=['that'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4z88oUR1Yk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Skipgram Training data \n",
        "\n",
        "# generate skip-grams with both positive and negative examples\n",
        "skip_grams = [skipgrams(sent, vocabulary_size=vocab_size, window_size=2) for sent in corpusByWordID]\n",
        "\n",
        "# view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "        vocab[pairs[i][0]], pairs[i][0],           \n",
        "        vocab[pairs[i][1]], pairs[i][1], \n",
        "        labels[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqD40Iq11fpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the skip-gram model\n",
        "\n",
        "#define the model\n",
        "input_word = Input((1,))\n",
        "input_context_word = Input((1,))\n",
        "\n",
        "word_embedding = Embedding(input_dim=vocab_size, output_dim=???,input_length=1,name='word_embedding')\n",
        "context_embedding = Embedding(input_dim=vocab_size, output_dim=???,input_length=1,name='conotext_embedding')\n",
        "\n",
        "word_embedding = word_embedding(input_word)\n",
        "word_embedding_layer = Reshape((embed_size, 1))(word_embedding)\n",
        "\n",
        "context_embedding = context_embedding(input_context_word)\n",
        "context_embedding_layer = Reshape((embed_size, 1))(context_embedding)\n",
        "\n",
        "# now perform the dot product operation word_embedding_vec * context_embedding_vec\n",
        "dot_product = dot([???, ???], axes=1)\n",
        "dot_product = Reshape((1,))(dot_product)\n",
        "\n",
        "###hint:the total number of possible labels/words\n",
        "###hint:activation='softmax' or 'sigmoid'\n",
        "outputLayer = Dense(???, activation='???')(dot_product)\n",
        "\n",
        "model = Model(input=[input_word, input_context_word], output=outputLayer)\n",
        "\n",
        "###hint:loss='categorical_crossentropy' or 'binary_crossentropy'\n",
        "model.compile(loss='???', optimizer='adam')\n",
        "\n",
        "# view model summary\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVpBqKfo1iSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the model\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "    loss = 0\n",
        "    for i, elem in enumerate(skip_grams):\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [pair_first_elem, pair_second_elem]\n",
        "        Y = labels\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjhkbLsp1k0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the embeding matrix\n",
        "weights = model.get_weights()\n",
        "## Save the wordvectors\n",
        "f = open('skipgram_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = model.get_weights()[0]\n",
        "for key in vocab:\n",
        "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
        "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4rUPCJC1mvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load the vectors back and validate\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./skipgram_vectors.txt', binary=False)\n",
        "w2v.most_similar(positive=['the'])\n",
        "w2v.most_similar(negative=['the'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs8ng_Zf1o08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Excerise: modeify the skipegram_model to share the same embeding layer between word and context\n",
        "#Discussion: which is better? Why?  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}