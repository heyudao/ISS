{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_workshop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_89yP2qzEui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ada08fa-c129-4693-c6b6-759c80a189dd"
      },
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install plot_keras_history\n",
        "!pip install seaborn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.31.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (49.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Collecting plot_keras_history\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/5a/8d60eae5d2624877fba35b63b70cea4edcf603c75883c004bb4715470d10/plot_keras_history-1.1.23.tar.gz\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (1.0.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (1.4.1)\n",
            "Collecting sanitize_ml_labels\n",
            "  Downloading https://files.pythonhosted.org/packages/69/2d/4db7caa3f08982c2ea48f6d6c19079246403a8742c14ea70875f48569dd6/sanitize_ml_labels-1.0.12.tar.gz\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->plot_keras_history) (2018.9)\n",
            "Collecting compress_json\n",
            "  Downloading https://files.pythonhosted.org/packages/c0/9d/1a79dbc3b9d69c57be363b4cc8bd6f278f919a0973f46c7bb831438c9333/compress_json-1.0.4.tar.gz\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->plot_keras_history) (1.15.0)\n",
            "Building wheels for collected packages: plot-keras-history, sanitize-ml-labels, compress-json\n",
            "  Building wheel for plot-keras-history (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for plot-keras-history: filename=plot_keras_history-1.1.23-cp36-none-any.whl size=6405 sha256=21d260131d4082ea0ec435c0a4275ca5e055740d7f8df33ce25cf901e26663e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/78/33/da5ed769fab5587fcdae95271e8d19106e3b92b3ae2d46382d\n",
            "  Building wheel for sanitize-ml-labels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sanitize-ml-labels: filename=sanitize_ml_labels-1.0.12-cp36-none-any.whl size=6596 sha256=b3312aaa79b6fdfa1b01f50a64ad6f40be25b0ed4856adf9797081fde3d10491\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/7a/44/401b091c00e8b2e314a474e3f7096d311239d95ff5315e25ab\n",
            "  Building wheel for compress-json (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compress-json: filename=compress_json-1.0.4-cp36-none-any.whl size=4587 sha256=824ddce7809bea0d9661c081a1ae8822d4833dc5b75f60c51540a4de9e1eb430\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/ec/21/51460dd508e4a152c0643946c21fae84eb3391171164d35745\n",
            "Successfully built plot-keras-history sanitize-ml-labels compress-json\n",
            "Installing collected packages: compress-json, sanitize-ml-labels, plot-keras-history\n",
            "Successfully installed compress-json-1.0.4 plot-keras-history-1.1.23 sanitize-ml-labels-1.0.12\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.0.5)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeiZ_Cqvzoqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda, Reshape\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import dot\n",
        "from tensorflow.keras.activations import relu\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "import gensim\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZazoZFH0BZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "32fca1ef-27b8-450d-d6ef-aa0e8ea549e6"
      },
      "source": [
        "# using nltk tokenizer.  \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co69rL7jzrh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0f849c8e-4a2e-4ad5-8c5b-f6dcffc4f041"
      },
      "source": [
        "#Data Preparation \n",
        "\n",
        "AlotOftext = \"\"\"Language users never choose words randomly, and language is essentially\n",
        "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
        "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
        "the null hypothesis will never be true. Moreover, where there is enough\n",
        "data, we shall (almost) always be able to establish that it is not true. In\n",
        "corpus studies, we frequently do have enough data, so the fact that a relation \n",
        "between two phenomena is demonstrably non-random, does not support the inference \n",
        "that it is not arbitrary. We present experimental evidence\n",
        "of how arbitrary associations between word frequencies and corpora are\n",
        "systematically non-random. We review literature in which hypothesis testing \n",
        "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
        "\n",
        "\n",
        "\n",
        "#Tokenize text\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(AlotOftext)]\n",
        "\n",
        "#Create Vocab as a Dictionary\n",
        "vocab = Dictionary(tokenized_text)\n",
        "print(dict(vocab.items()))\n",
        "\n",
        "print(vocab.token2id['corpora'])\n",
        "print(vocab[2])\n",
        "sent0 = tokenized_text[0]\n",
        "print(vocab.doc2idx(sent0))\n",
        "\n",
        "vocab.add_documents([['PAD']])\n",
        "dict(vocab.items())\n",
        "print(vocab.token2id['PAD'])\n",
        "\n",
        "corpusByWordID = list()\n",
        "for sent in  tokenized_text:\n",
        "    corpusByWordID.append(vocab.doc2idx(sent))\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 100\n",
        "window_size = 2 # context window size\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(vocab.items())[:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: ',', 1: '.', 2: 'and', 3: 'choose', 4: 'essentially', 5: 'is', 6: 'language', 7: 'never', 8: 'non-random', 9: 'randomly', 10: 'users', 11: 'words', 12: 'a', 13: 'hypothesis', 14: 'null', 15: 'posits', 16: 'randomness', 17: 'statistical', 18: 'testing', 19: 'uses', 20: 'which', 21: 'at', 22: 'be', 23: 'corpora', 24: 'hence', 25: 'in', 26: 'linguistic', 27: 'look', 28: 'phenomena', 29: 'the', 30: 'true', 31: 'we', 32: 'when', 33: 'will', 34: '(', 35: ')', 36: 'able', 37: 'almost', 38: 'always', 39: 'data', 40: 'enough', 41: 'establish', 42: 'it', 43: 'moreover', 44: 'not', 45: 'shall', 46: 'that', 47: 'there', 48: 'to', 49: 'where', 50: 'arbitrary', 51: 'between', 52: 'corpus', 53: 'demonstrably', 54: 'do', 55: 'does', 56: 'fact', 57: 'frequently', 58: 'have', 59: 'inference', 60: 'relation', 61: 'so', 62: 'studies', 63: 'support', 64: 'two', 65: 'are', 66: 'associations', 67: 'evidence', 68: 'experimental', 69: 'frequencies', 70: 'how', 71: 'of', 72: 'present', 73: 'systematically', 74: 'word', 75: 'been', 76: 'has', 77: 'led', 78: 'literature', 79: 'misleading', 80: 'often', 81: 'or', 82: 'results', 83: 'review', 84: 'show', 85: 'unhelpful', 86: 'used'}\n",
            "23\n",
            "and\n",
            "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]\n",
            "87\n",
            "Vocabulary Size: 88\n",
            "Vocabulary Sample: [(0, ','), (1, '.'), (2, 'and'), (3, 'choose'), (4, 'essentially'), (5, 'is'), (6, 'language'), (7, 'never'), (8, 'non-random'), (9, 'randomly')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBxqCqAL1BCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0c56880-31cc-47a1-8637-03d20f377a07"
      },
      "source": [
        "# Create CBOW Training data\n",
        "def generate_cbow_context_word_pairs(corpusByID, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for sent in corpusByID:\n",
        "        sentence_length = len(sent)\n",
        "        for index, word in enumerate(sent):\n",
        "            context_words = []\n",
        "            label_word   = []            \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            \n",
        "            context_words.append([sent[i] \n",
        "                                 for i in range(start, end) \n",
        "                                 if 0 <= i < sentence_length \n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "            if start<0:\n",
        "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='pre',value=vocab.token2id['PAD'])\n",
        "                y = np_utils.to_categorical(label_word, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "                continue\n",
        "            if end>=sentence_length:\n",
        "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='post',value=vocab.token2id['PAD'])\n",
        "                y = np_utils.to_categorical(label_word, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "                continue\n",
        "            else:\n",
        "                X.append(sequence.pad_sequences(context_words, maxlen=context_length))\n",
        "                Y.append(y)\n",
        "                continue\n",
        "           \n",
        "    return X,Y\n",
        "            \n",
        "# Test this out for some samples\n",
        "\n",
        "\n",
        "X,Y = generate_cbow_context_word_pairs(corpusByWordID, window_size, vocab_size) \n",
        "   \n",
        "for x, y in zip(X,Y):\n",
        "    print('Context (X):', [vocab[w] for w in x[0]], '-> Target (Y):', vocab[np.argwhere(y[0])[0][0]])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context (X): ['PAD', 'PAD', 'users', 'never'] -> Target (Y): language\n",
            "Context (X): ['PAD', 'language', 'never', 'choose'] -> Target (Y): users\n",
            "Context (X): ['language', 'users', 'choose', 'words'] -> Target (Y): users\n",
            "Context (X): ['users', 'never', 'words', 'randomly'] -> Target (Y): users\n",
            "Context (X): ['never', 'choose', 'randomly', ','] -> Target (Y): users\n",
            "Context (X): ['choose', 'words', ',', 'and'] -> Target (Y): users\n",
            "Context (X): ['words', 'randomly', 'and', 'language'] -> Target (Y): users\n",
            "Context (X): ['randomly', ',', 'language', 'is'] -> Target (Y): users\n",
            "Context (X): [',', 'and', 'is', 'essentially'] -> Target (Y): users\n",
            "Context (X): ['and', 'language', 'essentially', 'non-random'] -> Target (Y): users\n",
            "Context (X): ['language', 'is', 'non-random', '.'] -> Target (Y): essentially\n",
            "Context (X): ['is', 'essentially', '.', 'PAD'] -> Target (Y): non-random\n",
            "Context (X): ['essentially', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'hypothesis', 'testing'] -> Target (Y): statistical\n",
            "Context (X): ['PAD', 'statistical', 'testing', 'uses'] -> Target (Y): hypothesis\n",
            "Context (X): ['statistical', 'hypothesis', 'uses', 'a'] -> Target (Y): hypothesis\n",
            "Context (X): ['hypothesis', 'testing', 'a', 'null'] -> Target (Y): hypothesis\n",
            "Context (X): ['testing', 'uses', 'null', 'hypothesis'] -> Target (Y): hypothesis\n",
            "Context (X): ['uses', 'a', 'hypothesis', ','] -> Target (Y): hypothesis\n",
            "Context (X): ['a', 'null', ',', 'which'] -> Target (Y): hypothesis\n",
            "Context (X): ['null', 'hypothesis', 'which', 'posits'] -> Target (Y): hypothesis\n",
            "Context (X): ['hypothesis', ',', 'posits', 'randomness'] -> Target (Y): hypothesis\n",
            "Context (X): [',', 'which', 'randomness', '.'] -> Target (Y): posits\n",
            "Context (X): ['which', 'posits', '.', 'PAD'] -> Target (Y): randomness\n",
            "Context (X): ['posits', 'randomness', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', ',', 'when'] -> Target (Y): hence\n",
            "Context (X): ['PAD', 'hence', 'when', 'we'] -> Target (Y): ,\n",
            "Context (X): ['hence', ',', 'we', 'look'] -> Target (Y): ,\n",
            "Context (X): [',', 'when', 'look', 'at'] -> Target (Y): ,\n",
            "Context (X): ['when', 'we', 'at', 'linguistic'] -> Target (Y): ,\n",
            "Context (X): ['we', 'look', 'linguistic', 'phenomena'] -> Target (Y): ,\n",
            "Context (X): ['look', 'at', 'phenomena', 'in'] -> Target (Y): ,\n",
            "Context (X): ['at', 'linguistic', 'in', 'corpora'] -> Target (Y): ,\n",
            "Context (X): ['linguistic', 'phenomena', 'corpora', ','] -> Target (Y): ,\n",
            "Context (X): ['phenomena', 'in', ',', 'the'] -> Target (Y): ,\n",
            "Context (X): ['in', 'corpora', 'the', 'null'] -> Target (Y): ,\n",
            "Context (X): ['corpora', ',', 'null', 'hypothesis'] -> Target (Y): ,\n",
            "Context (X): [',', 'the', 'hypothesis', 'will'] -> Target (Y): ,\n",
            "Context (X): ['the', 'null', 'will', 'never'] -> Target (Y): ,\n",
            "Context (X): ['null', 'hypothesis', 'never', 'be'] -> Target (Y): ,\n",
            "Context (X): ['hypothesis', 'will', 'be', 'true'] -> Target (Y): ,\n",
            "Context (X): ['will', 'never', 'true', '.'] -> Target (Y): be\n",
            "Context (X): ['never', 'be', '.', 'PAD'] -> Target (Y): true\n",
            "Context (X): ['be', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', ',', 'where'] -> Target (Y): moreover\n",
            "Context (X): ['PAD', 'moreover', 'where', 'there'] -> Target (Y): ,\n",
            "Context (X): ['moreover', ',', 'there', 'is'] -> Target (Y): ,\n",
            "Context (X): [',', 'where', 'is', 'enough'] -> Target (Y): ,\n",
            "Context (X): ['where', 'there', 'enough', 'data'] -> Target (Y): ,\n",
            "Context (X): ['there', 'is', 'data', ','] -> Target (Y): ,\n",
            "Context (X): ['is', 'enough', ',', 'we'] -> Target (Y): ,\n",
            "Context (X): ['enough', 'data', 'we', 'shall'] -> Target (Y): ,\n",
            "Context (X): ['data', ',', 'shall', '('] -> Target (Y): ,\n",
            "Context (X): [',', 'we', '(', 'almost'] -> Target (Y): ,\n",
            "Context (X): ['we', 'shall', 'almost', ')'] -> Target (Y): ,\n",
            "Context (X): ['shall', '(', ')', 'always'] -> Target (Y): ,\n",
            "Context (X): ['(', 'almost', 'always', 'be'] -> Target (Y): ,\n",
            "Context (X): ['almost', ')', 'be', 'able'] -> Target (Y): ,\n",
            "Context (X): [')', 'always', 'able', 'to'] -> Target (Y): ,\n",
            "Context (X): ['always', 'be', 'to', 'establish'] -> Target (Y): ,\n",
            "Context (X): ['be', 'able', 'establish', 'that'] -> Target (Y): ,\n",
            "Context (X): ['able', 'to', 'that', 'it'] -> Target (Y): ,\n",
            "Context (X): ['to', 'establish', 'it', 'is'] -> Target (Y): ,\n",
            "Context (X): ['establish', 'that', 'is', 'not'] -> Target (Y): ,\n",
            "Context (X): ['that', 'it', 'not', 'true'] -> Target (Y): ,\n",
            "Context (X): ['it', 'is', 'true', '.'] -> Target (Y): not\n",
            "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): true\n",
            "Context (X): ['not', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'corpus', 'studies'] -> Target (Y): in\n",
            "Context (X): ['PAD', 'in', 'studies', ','] -> Target (Y): corpus\n",
            "Context (X): ['in', 'corpus', ',', 'we'] -> Target (Y): corpus\n",
            "Context (X): ['corpus', 'studies', 'we', 'frequently'] -> Target (Y): corpus\n",
            "Context (X): ['studies', ',', 'frequently', 'do'] -> Target (Y): corpus\n",
            "Context (X): [',', 'we', 'do', 'have'] -> Target (Y): corpus\n",
            "Context (X): ['we', 'frequently', 'have', 'enough'] -> Target (Y): corpus\n",
            "Context (X): ['frequently', 'do', 'enough', 'data'] -> Target (Y): corpus\n",
            "Context (X): ['do', 'have', 'data', ','] -> Target (Y): corpus\n",
            "Context (X): ['have', 'enough', ',', 'so'] -> Target (Y): corpus\n",
            "Context (X): ['enough', 'data', 'so', 'the'] -> Target (Y): corpus\n",
            "Context (X): ['data', ',', 'the', 'fact'] -> Target (Y): corpus\n",
            "Context (X): [',', 'so', 'fact', 'that'] -> Target (Y): corpus\n",
            "Context (X): ['so', 'the', 'that', 'a'] -> Target (Y): corpus\n",
            "Context (X): ['the', 'fact', 'a', 'relation'] -> Target (Y): corpus\n",
            "Context (X): ['fact', 'that', 'relation', 'between'] -> Target (Y): corpus\n",
            "Context (X): ['that', 'a', 'between', 'two'] -> Target (Y): corpus\n",
            "Context (X): ['a', 'relation', 'two', 'phenomena'] -> Target (Y): corpus\n",
            "Context (X): ['relation', 'between', 'phenomena', 'is'] -> Target (Y): corpus\n",
            "Context (X): ['between', 'two', 'is', 'demonstrably'] -> Target (Y): corpus\n",
            "Context (X): ['two', 'phenomena', 'demonstrably', 'non-random'] -> Target (Y): corpus\n",
            "Context (X): ['phenomena', 'is', 'non-random', ','] -> Target (Y): corpus\n",
            "Context (X): ['is', 'demonstrably', ',', 'does'] -> Target (Y): corpus\n",
            "Context (X): ['demonstrably', 'non-random', 'does', 'not'] -> Target (Y): corpus\n",
            "Context (X): ['non-random', ',', 'not', 'support'] -> Target (Y): corpus\n",
            "Context (X): [',', 'does', 'support', 'the'] -> Target (Y): corpus\n",
            "Context (X): ['does', 'not', 'the', 'inference'] -> Target (Y): corpus\n",
            "Context (X): ['not', 'support', 'inference', 'that'] -> Target (Y): corpus\n",
            "Context (X): ['support', 'the', 'that', 'it'] -> Target (Y): corpus\n",
            "Context (X): ['the', 'inference', 'it', 'is'] -> Target (Y): corpus\n",
            "Context (X): ['inference', 'that', 'is', 'not'] -> Target (Y): corpus\n",
            "Context (X): ['that', 'it', 'not', 'arbitrary'] -> Target (Y): corpus\n",
            "Context (X): ['it', 'is', 'arbitrary', '.'] -> Target (Y): not\n",
            "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): arbitrary\n",
            "Context (X): ['not', 'arbitrary', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'present', 'experimental'] -> Target (Y): we\n",
            "Context (X): ['PAD', 'we', 'experimental', 'evidence'] -> Target (Y): present\n",
            "Context (X): ['we', 'present', 'evidence', 'of'] -> Target (Y): present\n",
            "Context (X): ['present', 'experimental', 'of', 'how'] -> Target (Y): present\n",
            "Context (X): ['experimental', 'evidence', 'how', 'arbitrary'] -> Target (Y): present\n",
            "Context (X): ['evidence', 'of', 'arbitrary', 'associations'] -> Target (Y): present\n",
            "Context (X): ['of', 'how', 'associations', 'between'] -> Target (Y): present\n",
            "Context (X): ['how', 'arbitrary', 'between', 'word'] -> Target (Y): present\n",
            "Context (X): ['arbitrary', 'associations', 'word', 'frequencies'] -> Target (Y): present\n",
            "Context (X): ['associations', 'between', 'frequencies', 'and'] -> Target (Y): present\n",
            "Context (X): ['between', 'word', 'and', 'corpora'] -> Target (Y): present\n",
            "Context (X): ['word', 'frequencies', 'corpora', 'are'] -> Target (Y): present\n",
            "Context (X): ['frequencies', 'and', 'are', 'systematically'] -> Target (Y): present\n",
            "Context (X): ['and', 'corpora', 'systematically', 'non-random'] -> Target (Y): present\n",
            "Context (X): ['corpora', 'are', 'non-random', '.'] -> Target (Y): systematically\n",
            "Context (X): ['are', 'systematically', '.', 'PAD'] -> Target (Y): non-random\n",
            "Context (X): ['systematically', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'review', 'literature'] -> Target (Y): we\n",
            "Context (X): ['PAD', 'we', 'literature', 'in'] -> Target (Y): review\n",
            "Context (X): ['we', 'review', 'in', 'which'] -> Target (Y): review\n",
            "Context (X): ['review', 'literature', 'which', 'hypothesis'] -> Target (Y): review\n",
            "Context (X): ['literature', 'in', 'hypothesis', 'testing'] -> Target (Y): review\n",
            "Context (X): ['in', 'which', 'testing', 'has'] -> Target (Y): review\n",
            "Context (X): ['which', 'hypothesis', 'has', 'been'] -> Target (Y): review\n",
            "Context (X): ['hypothesis', 'testing', 'been', 'used'] -> Target (Y): review\n",
            "Context (X): ['testing', 'has', 'used', ','] -> Target (Y): review\n",
            "Context (X): ['has', 'been', ',', 'and'] -> Target (Y): review\n",
            "Context (X): ['been', 'used', 'and', 'show'] -> Target (Y): review\n",
            "Context (X): ['used', ',', 'show', 'how'] -> Target (Y): review\n",
            "Context (X): [',', 'and', 'how', 'it'] -> Target (Y): review\n",
            "Context (X): ['and', 'show', 'it', 'has'] -> Target (Y): review\n",
            "Context (X): ['show', 'how', 'has', 'often'] -> Target (Y): review\n",
            "Context (X): ['how', 'it', 'often', 'led'] -> Target (Y): review\n",
            "Context (X): ['it', 'has', 'led', 'to'] -> Target (Y): review\n",
            "Context (X): ['has', 'often', 'to', 'unhelpful'] -> Target (Y): review\n",
            "Context (X): ['often', 'led', 'unhelpful', 'or'] -> Target (Y): review\n",
            "Context (X): ['led', 'to', 'or', 'misleading'] -> Target (Y): review\n",
            "Context (X): ['to', 'unhelpful', 'misleading', 'results'] -> Target (Y): review\n",
            "Context (X): ['unhelpful', 'or', 'results', '.'] -> Target (Y): misleading\n",
            "Context (X): ['or', 'misleading', '.', 'PAD'] -> Target (Y): results\n",
            "Context (X): ['misleading', 'results', 'PAD', 'PAD'] -> Target (Y): .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH_iy4Lh1JUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "6cc6344b-878a-438f-d25e-66901623a74e"
      },
      "source": [
        "#define the model\n",
        "cbow = Sequential()\n",
        "###hint:output_dim = the shape of embedding matrix\n",
        "###hint:input_length = the length of training sample\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=1000))\n",
        "cbow.add(Lambda(lambda x: relu(K.mean(x, axis=1)), output_shape=(embed_size,)))\n",
        "###hint:the total numbser of possible labels/words\n",
        "###hint:activation='softmax' or 'sigmoid'\n",
        "cbow.add(Dense(88, activation='softmax'))\n",
        "###hint:loss='categorical_crossentropy' or 'binary_crossentropy'\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
        "cbow.summary()\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1000, 100)         8800      \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 88)                8888      \n",
            "=================================================================\n",
            "Total params: 17,688\n",
            "Trainable params: 17,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuVqi7WGDxNe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23048b3c-ab91-4b8b-816e-9d2f083fe1eb"
      },
      "source": [
        "#Train the model\n",
        "\n",
        "for epoch in range(1000):\n",
        "    loss = 0.\n",
        "    for x, y in zip(X,Y):\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "    print(epoch, loss)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(\"embedding_2_input:0\", shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (1, 4).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(\"embedding_2_input:0\", shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (1, 4).\n",
            "0 631.4400882720947\n",
            "1 604.3475439548492\n",
            "2 578.5113105773926\n",
            "3 554.1824760437012\n",
            "4 531.6189250946045\n",
            "5 511.0349793434143\n",
            "6 492.5742378234863\n",
            "7 476.2855553627014\n",
            "8 462.10508823394775\n",
            "9 449.8623617887497\n",
            "10 439.3364369869232\n",
            "11 430.2709106206894\n",
            "12 422.4196993112564\n",
            "13 415.5548372268677\n",
            "14 409.48642122745514\n",
            "15 404.0564351081848\n",
            "16 399.1408244371414\n",
            "17 394.63965129852295\n",
            "18 390.47264075279236\n",
            "19 386.5723046064377\n",
            "20 382.8869435787201\n",
            "21 379.3732533454895\n",
            "22 375.9937798976898\n",
            "23 372.7201807498932\n",
            "24 369.52367520332336\n",
            "25 366.38195073604584\n",
            "26 363.2725887298584\n",
            "27 360.18074691295624\n",
            "28 357.088960647583\n",
            "29 353.9818605184555\n",
            "30 350.8458740711212\n",
            "31 347.6740506887436\n",
            "32 344.44955348968506\n",
            "33 341.1670643091202\n",
            "34 337.8167779445648\n",
            "35 334.3973294496536\n",
            "36 330.8989690542221\n",
            "37 327.3278577327728\n",
            "38 323.6788774728775\n",
            "39 319.9549552202225\n",
            "40 316.15971636772156\n",
            "41 312.29038083553314\n",
            "42 308.3603159189224\n",
            "43 304.3673740029335\n",
            "44 300.32521879673004\n",
            "45 296.2412641644478\n",
            "46 292.1251283288002\n",
            "47 287.97854322195053\n",
            "48 283.80930560827255\n",
            "49 279.6256471276283\n",
            "50 275.4324080348015\n",
            "51 271.2321671843529\n",
            "52 267.030267059803\n",
            "53 262.8248979449272\n",
            "54 258.6228737235069\n",
            "55 254.42745888233185\n",
            "56 250.24637150764465\n",
            "57 246.07557916641235\n",
            "58 241.92647349834442\n",
            "59 237.80071383714676\n",
            "60 233.704919308424\n",
            "61 229.64755660295486\n",
            "62 225.62428188323975\n",
            "63 221.6505008637905\n",
            "64 217.72636541724205\n",
            "65 213.8520881831646\n",
            "66 210.04140648245811\n",
            "67 206.29321280121803\n",
            "68 202.60289034247398\n",
            "69 198.98464778065681\n",
            "70 195.42990577220917\n",
            "71 191.95147202908993\n",
            "72 188.54208533465862\n",
            "73 185.2053034156561\n",
            "74 181.94332386553288\n",
            "75 178.75099892914295\n",
            "76 175.63395662605762\n",
            "77 172.5932022333145\n",
            "78 169.62647895514965\n",
            "79 166.73198133707047\n",
            "80 163.91164304316044\n",
            "81 161.16349989175797\n",
            "82 158.48707726597786\n",
            "83 155.88050106167793\n",
            "84 153.34678742289543\n",
            "85 150.8838779553771\n",
            "86 148.48877501487732\n",
            "87 146.16126919537783\n",
            "88 143.89926056563854\n",
            "89 141.70326199382544\n",
            "90 139.57245952636003\n",
            "91 137.50470139086246\n",
            "92 135.49738719314337\n",
            "93 133.5514972731471\n",
            "94 131.66347474604845\n",
            "95 129.83489318192005\n",
            "96 128.06134817749262\n",
            "97 126.3451310172677\n",
            "98 124.68029337003827\n",
            "99 123.06678988784552\n",
            "100 121.50262649729848\n",
            "101 119.98864248022437\n",
            "102 118.52031674981117\n",
            "103 117.09594761580229\n",
            "104 115.71764994040132\n",
            "105 114.37963378056884\n",
            "106 113.08264576643705\n",
            "107 111.82292608171701\n",
            "108 110.60316522046924\n",
            "109 109.41749981418252\n",
            "110 108.26695395261049\n",
            "111 107.14716934040189\n",
            "112 106.06228087842464\n",
            "113 105.00282350368798\n",
            "114 103.97728382982314\n",
            "115 102.97644773684442\n",
            "116 102.0011729542166\n",
            "117 101.05275480262935\n",
            "118 100.12899564951658\n",
            "119 99.22624977305532\n",
            "120 98.34748882055283\n",
            "121 97.48963767290115\n",
            "122 96.65135159529746\n",
            "123 95.83457363396883\n",
            "124 95.03415956534445\n",
            "125 94.25225522182882\n",
            "126 93.4884987976402\n",
            "127 92.73994137533009\n",
            "128 92.00608091522008\n",
            "129 91.28995049092919\n",
            "130 90.58578524924815\n",
            "131 89.89610361400992\n",
            "132 89.21997962985188\n",
            "133 88.55679051205516\n",
            "134 87.90452893264592\n",
            "135 87.26643954217434\n",
            "136 86.63654569443315\n",
            "137 86.01881775259972\n",
            "138 85.41111505031586\n",
            "139 84.81332392804325\n",
            "140 84.22628814354539\n",
            "141 83.64550378639251\n",
            "142 83.07527726143599\n",
            "143 82.51380101591349\n",
            "144 81.9604865917936\n",
            "145 81.41475561726838\n",
            "146 80.8778436081484\n",
            "147 80.3457681555301\n",
            "148 79.82328002620488\n",
            "149 79.30489163985476\n",
            "150 78.7953815557994\n",
            "151 78.29033246217296\n",
            "152 77.79255843022838\n",
            "153 77.2998521081172\n",
            "154 76.81411057012156\n",
            "155 76.33293248899281\n",
            "156 75.8575251661241\n",
            "157 75.38764805905521\n",
            "158 74.92290669260547\n",
            "159 74.46253475826234\n",
            "160 74.00725037883967\n",
            "161 73.55629775300622\n",
            "162 73.11030576378107\n",
            "163 72.66835979511961\n",
            "164 72.23074425756931\n",
            "165 71.79656958812848\n",
            "166 71.36806646781042\n",
            "167 70.94229726074263\n",
            "168 70.52129555819556\n",
            "169 70.10294980695471\n",
            "170 69.68957672826946\n",
            "171 69.2795303822495\n",
            "172 68.87351466249675\n",
            "173 68.4695729566738\n",
            "174 68.07066895114258\n",
            "175 67.6737047159113\n",
            "176 67.28147491859272\n",
            "177 66.89142915257253\n",
            "178 66.50502461427823\n",
            "179 66.12048244266771\n",
            "180 65.74064885615371\n",
            "181 65.36168817826547\n",
            "182 64.98639620211907\n",
            "183 64.61462331051007\n",
            "184 64.24455296155065\n",
            "185 63.87690144311637\n",
            "186 63.51299855066463\n",
            "187 63.15150173334405\n",
            "188 62.79144882434048\n",
            "189 62.435497991973534\n",
            "190 62.08052163710818\n",
            "191 61.72904417105019\n",
            "192 61.37911253422499\n",
            "193 61.03305579395965\n",
            "194 60.68747984035872\n",
            "195 60.34481525118463\n",
            "196 60.00452417926863\n",
            "197 59.66715481993742\n",
            "198 59.330305566545576\n",
            "199 58.99743488128297\n",
            "200 58.66463941358961\n",
            "201 58.336011751554906\n",
            "202 58.008486941689625\n",
            "203 57.682435964932665\n",
            "204 57.35847453563474\n",
            "205 57.037509583635256\n",
            "206 56.71786802727729\n",
            "207 56.39957555336878\n",
            "208 56.084620520239696\n",
            "209 55.76993220159784\n",
            "210 55.457882550079376\n",
            "211 55.14764359185938\n",
            "212 54.838135002413765\n",
            "213 54.532163729309104\n",
            "214 54.2258519011084\n",
            "215 53.92326086759567\n",
            "216 53.62076847394928\n",
            "217 53.32063035981264\n",
            "218 53.0224615681218\n",
            "219 52.725426212884486\n",
            "220 52.430142472847365\n",
            "221 52.13638833875302\n",
            "222 51.84438938787207\n",
            "223 51.55486847925931\n",
            "224 51.26528831629548\n",
            "225 50.977984234225005\n",
            "226 50.692831312539056\n",
            "227 50.40771040681284\n",
            "228 50.125067356275395\n",
            "229 49.844166299677454\n",
            "230 49.564200709806755\n",
            "231 49.28573373192921\n",
            "232 49.00907153671142\n",
            "233 48.73283099709079\n",
            "234 48.459710871917196\n",
            "235 48.18623661866877\n",
            "236 47.915668725501746\n",
            "237 47.64578670554329\n",
            "238 47.377015993813984\n",
            "239 47.11053481895942\n",
            "240 46.84473495488055\n",
            "241 46.580538287293166\n",
            "242 46.31701223226264\n",
            "243 46.055401226039976\n",
            "244 45.795261941617355\n",
            "245 45.53558783954941\n",
            "246 45.27789163996931\n",
            "247 45.02153039863333\n",
            "248 44.765381574165076\n",
            "249 44.511144948075525\n",
            "250 44.25858328200411\n",
            "251 44.0061600379413\n",
            "252 43.7560713798739\n",
            "253 43.50648065819405\n",
            "254 43.258787568309344\n",
            "255 43.01224700862076\n",
            "256 42.76655467413366\n",
            "257 42.52182883571368\n",
            "258 42.279531933774706\n",
            "259 42.037375956133474\n",
            "260 41.797156526881736\n",
            "261 41.55731953645591\n",
            "262 41.319577625370584\n",
            "263 41.08165642764652\n",
            "264 40.84609651088249\n",
            "265 40.611315091489814\n",
            "266 40.3775623444817\n",
            "267 40.1449401389109\n",
            "268 39.91380879149074\n",
            "269 39.68344611703651\n",
            "270 39.454196568985935\n",
            "271 39.225891867419705\n",
            "272 38.999401330947876\n",
            "273 38.772992707497906\n",
            "274 38.548199904267676\n",
            "275 38.324672001414\n",
            "276 38.101769365312066\n",
            "277 37.88032053440111\n",
            "278 37.65935041796183\n",
            "279 37.44039470242569\n",
            "280 37.22179393476108\n",
            "281 37.00496550468961\n",
            "282 36.788556987419724\n",
            "283 36.57354280585423\n",
            "284 36.35873829695629\n",
            "285 36.146422279882245\n",
            "286 35.93358034454286\n",
            "287 35.72279859299306\n",
            "288 35.51375712105073\n",
            "289 35.30414697516244\n",
            "290 35.09718174574664\n",
            "291 34.889925699157175\n",
            "292 34.68520164646907\n",
            "293 34.48031133261975\n",
            "294 34.27675908623496\n",
            "295 34.07490706781391\n",
            "296 33.873356108611915\n",
            "297 33.672908791399095\n",
            "298 33.474086299480405\n",
            "299 33.275340857275296\n",
            "300 33.07884299359284\n",
            "301 32.882169144111685\n",
            "302 32.68770017114002\n",
            "303 32.493647038470954\n",
            "304 32.300459188059904\n",
            "305 32.1086196784745\n",
            "306 31.91749327816069\n",
            "307 31.727150451624766\n",
            "308 31.538471638807096\n",
            "309 31.350198494270444\n",
            "310 31.163227503187954\n",
            "311 30.97758097195765\n",
            "312 30.79177051439183\n",
            "313 30.608700796205085\n",
            "314 30.42572204762837\n",
            "315 30.243121711653657\n",
            "316 30.062685443728697\n",
            "317 29.88249478454236\n",
            "318 29.70308386249235\n",
            "319 29.52611236879602\n",
            "320 29.34809485534788\n",
            "321 29.172800946776988\n",
            "322 28.997839918214595\n",
            "323 28.823715085774893\n",
            "324 28.651017997413874\n",
            "325 28.478688588395016\n",
            "326 28.307608209812315\n",
            "327 28.13731059350539\n",
            "328 27.969087876204867\n",
            "329 27.80015499572619\n",
            "330 27.63323017102084\n",
            "331 27.467026064230595\n",
            "332 27.302107319876086\n",
            "333 27.137136459408794\n",
            "334 26.973983742529526\n",
            "335 26.811577271611895\n",
            "336 26.651242295920383\n",
            "337 26.48985367800924\n",
            "338 26.331225488014752\n",
            "339 26.172205277049216\n",
            "340 26.014804638485657\n",
            "341 25.858329898968805\n",
            "342 25.702269634086406\n",
            "343 25.547805554117076\n",
            "344 25.394023523374926\n",
            "345 25.240367275371682\n",
            "346 25.08935508725699\n",
            "347 24.93792296756874\n",
            "348 24.78801162331365\n",
            "349 24.638444506708765\n",
            "350 24.49019314791076\n",
            "351 24.343574072554475\n",
            "352 24.19582554034423\n",
            "353 24.05143025011057\n",
            "354 23.906048331031343\n",
            "355 23.76251892655273\n",
            "356 23.61985983638442\n",
            "357 23.477921557059744\n",
            "358 23.336880355869653\n",
            "359 23.19649262042367\n",
            "360 23.05692225904204\n",
            "361 22.919214071473107\n",
            "362 22.781735973840114\n",
            "363 22.64488090958912\n",
            "364 22.509181111759972\n",
            "365 22.37473202883848\n",
            "366 22.240734775463352\n",
            "367 22.10746354373987\n",
            "368 21.97564665073878\n",
            "369 21.84458259161329\n",
            "370 21.71427961430163\n",
            "371 21.584993353841128\n",
            "372 21.45653233086341\n",
            "373 21.3287826385058\n",
            "374 21.20149945121375\n",
            "375 21.07587572335615\n",
            "376 20.951071422867244\n",
            "377 20.82643671700498\n",
            "378 20.70360451019951\n",
            "379 20.581415198132163\n",
            "380 20.459391082724323\n",
            "381 20.338859847630374\n",
            "382 20.218733659829013\n",
            "383 20.099422708764905\n",
            "384 19.98150844516931\n",
            "385 19.86414654608234\n",
            "386 19.74711249273969\n",
            "387 19.63136273136479\n",
            "388 19.51641005941201\n",
            "389 19.402118915721076\n",
            "390 19.28851050575031\n",
            "391 19.176272619399242\n",
            "392 19.06447406791267\n",
            "393 18.953375428682193\n",
            "394 18.842954338586424\n",
            "395 18.733847757743206\n",
            "396 18.625185333832633\n",
            "397 18.517241616005776\n",
            "398 18.410073995008133\n",
            "399 18.304008443752537\n",
            "400 18.198414834885625\n",
            "401 18.093255580490222\n",
            "402 17.989941106701735\n",
            "403 17.887049654120347\n",
            "404 17.78388536478451\n",
            "405 17.68281148580718\n",
            "406 17.5818788556644\n",
            "407 17.48172289264039\n",
            "408 17.382153957951232\n",
            "409 17.283803954152972\n",
            "410 17.185245164568187\n",
            "411 17.088322706418694\n",
            "412 16.991635630707606\n",
            "413 16.89624632849882\n",
            "414 16.801378553165705\n",
            "415 16.706822324340465\n",
            "416 16.6132348539104\n",
            "417 16.519949488749262\n",
            "418 16.428558219820843\n",
            "419 16.336622504342813\n",
            "420 16.24592490082432\n",
            "421 16.15582349880424\n",
            "422 16.066437615649193\n",
            "423 15.977400734671392\n",
            "424 15.889869361926685\n",
            "425 15.802417617000174\n",
            "426 15.715214195428416\n",
            "427 15.62921902447124\n",
            "428 15.54415657860227\n",
            "429 15.459636117273476\n",
            "430 15.375302001906675\n",
            "431 15.291846952794003\n",
            "432 15.208877297642175\n",
            "433 15.12731900192739\n",
            "434 15.045574247065815\n",
            "435 14.964759224865702\n",
            "436 14.884458935659495\n",
            "437 14.805303072585957\n",
            "438 14.7261209571152\n",
            "439 14.648032870638417\n",
            "440 14.570291686119162\n",
            "441 14.493053257378051\n",
            "442 14.41701909195399\n",
            "443 14.340919661044609\n",
            "444 14.265641260135453\n",
            "445 14.19088061110233\n",
            "446 14.116723436745815\n",
            "447 14.043371014297009\n",
            "448 13.970581196947023\n",
            "449 13.897976898850175\n",
            "450 13.826121706690174\n",
            "451 13.755004926992115\n",
            "452 13.684620752334013\n",
            "453 13.614287907475955\n",
            "454 13.544863648101455\n",
            "455 13.475636050789035\n",
            "456 13.40743370668497\n",
            "457 13.339339016660233\n",
            "458 13.271926973349764\n",
            "459 13.204797126309131\n",
            "460 13.138758098124526\n",
            "461 13.072802466005669\n",
            "462 13.007863019389333\n",
            "463 12.942482470563846\n",
            "464 12.878451946235145\n",
            "465 12.814785590831889\n",
            "466 12.751434470163076\n",
            "467 12.688623414811445\n",
            "468 12.626329231934506\n",
            "469 12.564533751356066\n",
            "470 12.503088431083597\n",
            "471 12.442680521533475\n",
            "472 12.381903557063197\n",
            "473 12.322615766039235\n",
            "474 12.262431561015546\n",
            "475 12.204169391901814\n",
            "476 12.14538581117813\n",
            "477 12.087426707686973\n",
            "478 12.030213817895856\n",
            "479 11.972650897325366\n",
            "480 11.915996356547112\n",
            "481 11.860118572672945\n",
            "482 11.804246878571576\n",
            "483 11.749015093257185\n",
            "484 11.69379823836789\n",
            "485 11.639725586326676\n",
            "486 11.585589790469385\n",
            "487 11.532241900407826\n",
            "488 11.478507391395397\n",
            "489 11.426168646343285\n",
            "490 11.373917043907568\n",
            "491 11.321584860706935\n",
            "492 11.270151260003331\n",
            "493 11.2190817004157\n",
            "494 11.168308864856954\n",
            "495 11.11781239658012\n",
            "496 11.067654653379577\n",
            "497 11.018303126693354\n",
            "498 10.969421737419907\n",
            "499 10.920032909954898\n",
            "500 10.871806250477675\n",
            "501 10.823781346756732\n",
            "502 10.776250897877617\n",
            "503 10.728375455437344\n",
            "504 10.68166196923994\n",
            "505 10.635099664577865\n",
            "506 10.58879940638144\n",
            "507 10.542898705520201\n",
            "508 10.497041705195443\n",
            "509 10.452323479228653\n",
            "510 10.407492182290298\n",
            "511 10.362517487046716\n",
            "512 10.318430040068051\n",
            "513 10.274647580816236\n",
            "514 10.231289323739475\n",
            "515 10.1880238395388\n",
            "516 10.145112816469918\n",
            "517 10.102475646570383\n",
            "518 10.06032519719156\n",
            "519 10.018504657622543\n",
            "520 9.97663484710938\n",
            "521 9.93542682743282\n",
            "522 9.894661127014842\n",
            "523 9.85354014330369\n",
            "524 9.813423031220736\n",
            "525 9.773137066629715\n",
            "526 9.733692695335776\n",
            "527 9.693932336034777\n",
            "528 9.654826096215402\n",
            "529 9.616199217212852\n",
            "530 9.577317955285253\n",
            "531 9.538939795646002\n",
            "532 9.50094970921782\n",
            "533 9.463293768785661\n",
            "534 9.425717041020107\n",
            "535 9.388385350284807\n",
            "536 9.351595550469938\n",
            "537 9.314871204805968\n",
            "538 9.278276709745114\n",
            "539 9.242237755992392\n",
            "540 9.206356545895687\n",
            "541 9.170518630591687\n",
            "542 9.13517238577333\n",
            "543 9.100047538675426\n",
            "544 9.065038788539823\n",
            "545 9.030550882358511\n",
            "546 8.996050061366986\n",
            "547 8.962041795995901\n",
            "548 8.928232376463711\n",
            "549 8.894524941992131\n",
            "550 8.861090707752737\n",
            "551 8.8279479655248\n",
            "552 8.795253805110406\n",
            "553 8.762277354297112\n",
            "554 8.729942526995728\n",
            "555 8.697585473171785\n",
            "556 8.665899599225668\n",
            "557 8.633718327255337\n",
            "558 8.602339925811975\n",
            "559 8.570987379272992\n",
            "560 8.539759861720086\n",
            "561 8.508880436427717\n",
            "562 8.478012752377253\n",
            "563 8.447633162555576\n",
            "564 8.417146131163463\n",
            "565 8.387352889330941\n",
            "566 8.357456842168176\n",
            "567 8.327699124718492\n",
            "568 8.298563629490673\n",
            "569 8.269243990864197\n",
            "570 8.239953043274\n",
            "571 8.211505242114072\n",
            "572 8.182232034116169\n",
            "573 8.154374964135059\n",
            "574 8.125759122413001\n",
            "575 8.097883475595154\n",
            "576 8.070119704803801\n",
            "577 8.042156103867455\n",
            "578 8.014801132449065\n",
            "579 7.98712095884548\n",
            "580 7.960498317392194\n",
            "581 7.933200287006912\n",
            "582 7.906687619688455\n",
            "583 7.879960285848938\n",
            "584 7.8532293557946105\n",
            "585 7.827459246240323\n",
            "586 7.800851768486609\n",
            "587 7.775362211359607\n",
            "588 7.749529295018874\n",
            "589 7.723830615999759\n",
            "590 7.698401405890763\n",
            "591 7.6732379115055664\n",
            "592 7.64832202764228\n",
            "593 7.622970939388324\n",
            "594 7.598645854748611\n",
            "595 7.573939578469435\n",
            "596 7.549282290201518\n",
            "597 7.524951130879344\n",
            "598 7.5012516665156\n",
            "599 7.477111960295588\n",
            "600 7.453076839839923\n",
            "601 7.429760168946814\n",
            "602 7.406305834942032\n",
            "603 7.382443160924595\n",
            "604 7.3593295882310485\n",
            "605 7.3365502348024165\n",
            "606 7.313763230697077\n",
            "607 7.290453687623085\n",
            "608 7.268217800243292\n",
            "609 7.245533388464537\n",
            "610 7.223333587535308\n",
            "611 7.200823463157576\n",
            "612 7.179158798287972\n",
            "613 7.157192742532061\n",
            "614 7.135125494802196\n",
            "615 7.113602283250657\n",
            "616 7.091911797760986\n",
            "617 7.070769921097963\n",
            "618 7.049446993332822\n",
            "619 7.028141465416411\n",
            "620 7.0073274048554595\n",
            "621 6.986227976507507\n",
            "622 6.965706616945681\n",
            "623 6.944725779547298\n",
            "624 6.924717538604455\n",
            "625 6.904311153848539\n",
            "626 6.883527432953997\n",
            "627 6.863970693775627\n",
            "628 6.84360917093727\n",
            "629 6.823832303518429\n",
            "630 6.804008277846151\n",
            "631 6.784476201581128\n",
            "632 6.765004279310233\n",
            "633 6.745254857305554\n",
            "634 6.726155333111819\n",
            "635 6.7068355229566805\n",
            "636 6.687673867287231\n",
            "637 6.669038498635928\n",
            "638 6.64988085773075\n",
            "639 6.631283588816586\n",
            "640 6.6126278461451875\n",
            "641 6.594273465205333\n",
            "642 6.5758786814622\n",
            "643 6.557617826474598\n",
            "644 6.539297905597778\n",
            "645 6.521472772830748\n",
            "646 6.503465091584076\n",
            "647 6.485227497432788\n",
            "648 6.467933935346082\n",
            "649 6.449885160051053\n",
            "650 6.432480630930513\n",
            "651 6.414895687892567\n",
            "652 6.3976770474691875\n",
            "653 6.380512594681932\n",
            "654 6.362946801033104\n",
            "655 6.346329677398899\n",
            "656 6.329064147459576\n",
            "657 6.312333043781109\n",
            "658 6.2953589104399725\n",
            "659 6.2788828459633805\n",
            "660 6.26233130466062\n",
            "661 6.245470702921011\n",
            "662 6.229395175902027\n",
            "663 6.213018746118905\n",
            "664 6.196808227323345\n",
            "665 6.1804779273224995\n",
            "666 6.164757658028975\n",
            "667 6.148843569135352\n",
            "668 6.132481349613954\n",
            "669 6.117221558215533\n",
            "670 6.101079995136388\n",
            "671 6.085809550950216\n",
            "672 6.069961349305231\n",
            "673 6.054958215172519\n",
            "674 6.039282254168938\n",
            "675 6.024249333302578\n",
            "676 6.009133609168202\n",
            "677 5.9936695814431005\n",
            "678 5.978936346466071\n",
            "679 5.963651741880312\n",
            "680 5.949270611145039\n",
            "681 5.934141332738363\n",
            "682 5.919543199332111\n",
            "683 5.905082685785601\n",
            "684 5.890291676136258\n",
            "685 5.876083563078282\n",
            "686 5.861378857869568\n",
            "687 5.847465784594533\n",
            "688 5.8327980860485695\n",
            "689 5.818843347202346\n",
            "690 5.805058007714251\n",
            "691 5.790551018391852\n",
            "692 5.776618333096849\n",
            "693 5.762925756225741\n",
            "694 5.748993559052906\n",
            "695 5.7352700006376836\n",
            "696 5.721650917232182\n",
            "697 5.708084566536854\n",
            "698 5.694614940723113\n",
            "699 5.681076962770021\n",
            "700 5.667903683646728\n",
            "701 5.654811251235515\n",
            "702 5.641674849215633\n",
            "703 5.628169186809828\n",
            "704 5.615470237964473\n",
            "705 5.602126302113902\n",
            "706 5.589443176166242\n",
            "707 5.57611451273624\n",
            "708 5.563924853933713\n",
            "709 5.551155159999325\n",
            "710 5.5380120115987665\n",
            "711 5.525821473547694\n",
            "712 5.512893334864202\n",
            "713 5.500650837973808\n",
            "714 5.488048206181702\n",
            "715 5.476019782545336\n",
            "716 5.463693578840321\n",
            "717 5.451039296433009\n",
            "718 5.438870251833578\n",
            "719 5.427164196436934\n",
            "720 5.415148198149836\n",
            "721 5.402713620213035\n",
            "722 5.391208612763876\n",
            "723 5.379276542847947\n",
            "724 5.367125465254503\n",
            "725 5.35530932441543\n",
            "726 5.344074500248098\n",
            "727 5.332351390195981\n",
            "728 5.3204947795202315\n",
            "729 5.309213900083705\n",
            "730 5.2974024452705635\n",
            "731 5.286462497850152\n",
            "732 5.2750351556169335\n",
            "733 5.263327355372894\n",
            "734 5.252483075768396\n",
            "735 5.2409217576096125\n",
            "736 5.230077006908687\n",
            "737 5.218693003098451\n",
            "738 5.208073937777954\n",
            "739 5.197013209501165\n",
            "740 5.185722934566002\n",
            "741 5.174910313406144\n",
            "742 5.164370318736474\n",
            "743 5.153587954442628\n",
            "744 5.142577630722371\n",
            "745 5.132247224413732\n",
            "746 5.121351790156041\n",
            "747 5.111139193788404\n",
            "748 5.1004777205853316\n",
            "749 5.089760461098194\n",
            "750 5.079699109730427\n",
            "751 5.06885352255631\n",
            "752 5.059110117948876\n",
            "753 5.0483511389466\n",
            "754 5.038472694002849\n",
            "755 5.028337360876321\n",
            "756 5.017860007792478\n",
            "757 5.007858927292546\n",
            "758 4.998077716980333\n",
            "759 4.988129375946301\n",
            "760 4.977856507004617\n",
            "761 4.968250490328501\n",
            "762 4.9580690029179095\n",
            "763 4.948720286782191\n",
            "764 4.938499695206701\n",
            "765 4.929200100265007\n",
            "766 4.919579265908396\n",
            "767 4.909554334910354\n",
            "768 4.900311690904346\n",
            "769 4.890468492663786\n",
            "770 4.881358761133015\n",
            "771 4.87181198718099\n",
            "772 4.86212550408527\n",
            "773 4.853094121110189\n",
            "774 4.843509199221444\n",
            "775 4.83444639087611\n",
            "776 4.824992675137764\n",
            "777 4.816132841144281\n",
            "778 4.80692508303764\n",
            "779 4.797535062825773\n",
            "780 4.788444242927653\n",
            "781 4.779808930852596\n",
            "782 4.770775689881702\n",
            "783 4.761468701392005\n",
            "784 4.752956847758469\n",
            "785 4.743770092667546\n",
            "786 4.73515490724094\n",
            "787 4.726618191612943\n",
            "788 4.717385805543017\n",
            "789 4.708927386036521\n",
            "790 4.700131540364964\n",
            "791 4.6917574493345455\n",
            "792 4.682747622271563\n",
            "793 4.674615707466728\n",
            "794 4.666159371721733\n",
            "795 4.6571435926853155\n",
            "796 4.6488338483759435\n",
            "797 4.640781282334501\n",
            "798 4.632468894928024\n",
            "799 4.623731051775394\n",
            "800 4.615787563274353\n",
            "801 4.607133915989834\n",
            "802 4.599379611325276\n",
            "803 4.590754564032977\n",
            "804 4.582681571977446\n",
            "805 4.574540559133311\n",
            "806 4.566373067784298\n",
            "807 4.558394989537192\n",
            "808 4.550366635317914\n",
            "809 4.5425050416524755\n",
            "810 4.53445598140388\n",
            "811 4.5265505879615375\n",
            "812 4.518669140041311\n",
            "813 4.510908573458437\n",
            "814 4.502990224711539\n",
            "815 4.49569885528399\n",
            "816 4.487563385402609\n",
            "817 4.480225315979624\n",
            "818 4.472553348878137\n",
            "819 4.464521957466786\n",
            "820 4.4569857548085565\n",
            "821 4.449731580250955\n",
            "822 4.442208904085419\n",
            "823 4.434301967201463\n",
            "824 4.427272126067692\n",
            "825 4.419336524526443\n",
            "826 4.412424395566632\n",
            "827 4.404620358745888\n",
            "828 4.397584186972381\n",
            "829 4.390277593793144\n",
            "830 4.382644489309314\n",
            "831 4.375851380857057\n",
            "832 4.368155237345491\n",
            "833 4.361286061153805\n",
            "834 4.353810633470857\n",
            "835 4.347027045201685\n",
            "836 4.339415185426333\n",
            "837 4.332922715981113\n",
            "838 4.325813007148099\n",
            "839 4.3183105060015805\n",
            "840 4.311781283962773\n",
            "841 4.304463738855702\n",
            "842 4.297930370335962\n",
            "843 4.290604866309877\n",
            "844 4.284025955348625\n",
            "845 4.277294527339109\n",
            "846 4.270024496767292\n",
            "847 4.263186606909585\n",
            "848 4.256895794907905\n",
            "849 4.250202574276045\n",
            "850 4.24299758539928\n",
            "851 4.236743853343796\n",
            "852 4.229636566084082\n",
            "853 4.22351871310093\n",
            "854 4.216474551671126\n",
            "855 4.210263706117985\n",
            "856 4.203757138377114\n",
            "857 4.196837853098259\n",
            "858 4.190667710216076\n",
            "859 4.183844989016507\n",
            "860 4.177853645607684\n",
            "861 4.170936116279336\n",
            "862 4.1650035390448465\n",
            "863 4.158611003767874\n",
            "864 4.151929123006994\n",
            "865 4.146005605687606\n",
            "866 4.139215083871022\n",
            "867 4.133485494878187\n",
            "868 4.126703614621874\n",
            "869 4.120926014626093\n",
            "870 4.114375268049116\n",
            "871 4.108628769385177\n",
            "872 4.102461127220522\n",
            "873 4.095969559455625\n",
            "874 4.090182606054441\n",
            "875 4.083803149798769\n",
            "876 4.078180262014939\n",
            "877 4.071628596895607\n",
            "878 4.066112430224166\n",
            "879 4.059736440583947\n",
            "880 4.054139556374139\n",
            "881 4.047858566160357\n",
            "882 4.042395173402838\n",
            "883 4.03644488815371\n",
            "884 4.030226809505621\n",
            "885 4.024699432620764\n",
            "886 4.0185671449635265\n",
            "887 4.013199331096985\n",
            "888 4.006913401466591\n",
            "889 4.001626528406632\n",
            "890 3.9959907671618566\n",
            "891 3.989739098109567\n",
            "892 3.984593429009692\n",
            "893 3.978450587388579\n",
            "894 3.973233552142119\n",
            "895 3.9672228264880687\n",
            "896 3.9620155713982967\n",
            "897 3.9560311797704344\n",
            "898 3.9509428939618374\n",
            "899 3.9453384184034803\n",
            "900 3.9394545709419617\n",
            "901 3.9339769150010397\n",
            "902 3.92848424305339\n",
            "903 3.9231043217114347\n",
            "904 3.917625832509657\n",
            "905 3.91220957616315\n",
            "906 3.906848751470534\n",
            "907 3.9014713655469677\n",
            "908 3.8961089987224113\n",
            "909 3.8907795742325106\n",
            "910 3.8854636086780374\n",
            "911 3.880265627250992\n",
            "912 3.8748968887484807\n",
            "913 3.8696383747883374\n",
            "914 3.8644938517409173\n",
            "915 3.859281709721472\n",
            "916 3.8544783746274334\n",
            "917 3.8494044249691797\n",
            "918 3.8437356621416257\n",
            "919 3.8385824767447048\n",
            "920 3.833965645493663\n",
            "921 3.8288820826401206\n",
            "922 3.823423610503596\n",
            "923 3.818764103241847\n",
            "924 3.813265707844039\n",
            "925 3.8087380172164558\n",
            "926 3.8032997091413563\n",
            "927 3.79870260192547\n",
            "928 3.7937912485758716\n",
            "929 3.7883967630841653\n",
            "930 3.783934923550987\n",
            "931 3.7785480280508636\n",
            "932 3.7741721550773946\n",
            "933 3.7688418929756153\n",
            "934 3.7644525154919393\n",
            "935 3.759089796762055\n",
            "936 3.7547852360967227\n",
            "937 3.749988733276041\n",
            "938 3.7447448439288564\n",
            "939 3.7404701331888646\n",
            "940 3.7352592051192914\n",
            "941 3.730971819984916\n",
            "942 3.7258126547149004\n",
            "943 3.721594976263077\n",
            "944 3.716931764753099\n",
            "945 3.7118032641883474\n",
            "946 3.7070682053908968\n",
            "947 3.702971091714062\n",
            "948 3.6983782477091154\n",
            "949 3.6932872719899024\n",
            "950 3.6892275758054893\n",
            "951 3.6841351780403784\n",
            "952 3.6801338858094823\n",
            "953 3.675060475470673\n",
            "954 3.6710219725337083\n",
            "955 3.6665981555051985\n",
            "956 3.6616764378159132\n",
            "957 3.6575788314512465\n",
            "958 3.6527146945809363\n",
            "959 3.6487541334736306\n",
            "960 3.643816488720404\n",
            "961 3.6399263288549264\n",
            "962 3.635067944171169\n",
            "963 3.6311661914587603\n",
            "964 3.6267995160269493\n",
            "965 3.6220142465244862\n",
            "966 3.6181827543150575\n",
            "967 3.6134093058335566\n",
            "968 3.609525982725245\n",
            "969 3.6048110309329786\n",
            "970 3.6009950860952813\n",
            "971 3.596796369947697\n",
            "972 3.5921109454557154\n",
            "973 3.587846616288516\n",
            "974 3.584085046290056\n",
            "975 3.579898745056198\n",
            "976 3.575275580154994\n",
            "977 3.571562033774171\n",
            "978 3.566990338049436\n",
            "979 3.5633126070006256\n",
            "980 3.5586723190044722\n",
            "981 3.555086213795221\n",
            "982 3.551020984781644\n",
            "983 3.5464551124132413\n",
            "984 3.5423782069028675\n",
            "985 3.5384014174014737\n",
            "986 3.534337587409027\n",
            "987 3.530309752235553\n",
            "988 3.5263350266777707\n",
            "989 3.522315248656014\n",
            "990 3.5183645170582167\n",
            "991 3.514318099563752\n",
            "992 3.510415979262689\n",
            "993 3.5065179293669644\n",
            "994 3.502564223306763\n",
            "995 3.4986257704567834\n",
            "996 3.494734843252445\n",
            "997 3.490862168777312\n",
            "998 3.486954488203992\n",
            "999 3.4831450990277517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxPVT_G-1RYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save the wordvectors\n",
        "f = open('Cbow_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = cbow.get_weights()[0]\n",
        "for key in vocab:\n",
        "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
        "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
        "f.close()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1H3zTpE1Uwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "181a8195-1af8-48e3-cf1f-e8d19dbc0833"
      },
      "source": [
        "## Load the vectors back and validate\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./Cbow_vectors.txt', binary=False)\n",
        "\n",
        "w2v.most_similar(positive=['that'])\n",
        "w2v.most_similar(negative=['that'])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('essentially', 0.41536492109298706),\n",
              " ('how', 0.3250637650489807),\n",
              " ('and', 0.32494449615478516),\n",
              " ('used', 0.29641997814178467),\n",
              " ('literature', 0.2688229978084564),\n",
              " ('review', 0.26424601674079895),\n",
              " ('systematically', 0.2554593086242676),\n",
              " ('which', 0.23558300733566284),\n",
              " ('randomly', 0.23268665373325348),\n",
              " ('misleading', 0.22910144925117493)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4z88oUR1Yk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "e281566f-ae5c-4a96-af9c-ea5038b680ea"
      },
      "source": [
        "#Create Skipgram Training data \n",
        "\n",
        "# generate skip-grams with both positive and negative examples\n",
        "skip_grams = [skipgrams(sent, vocabulary_size=vocab_size, window_size=2) for sent in corpusByWordID]\n",
        "\n",
        "# view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "        vocab[pairs[i][0]], pairs[i][0],           \n",
        "        vocab[pairs[i][1]], pairs[i][1], \n",
        "        labels[i]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(never (7), been (75)) -> 0\n",
            "(randomly (9), choose (3)) -> 0\n",
            "(essentially (4), non-random (8)) -> 1\n",
            "(non-random (8), essentially (4)) -> 1\n",
            "(essentially (4), language (6)) -> 1\n",
            "(randomly (9), are (65)) -> 0\n",
            "(users (10), hence (24)) -> 0\n",
            "(words (11), testing (18)) -> 0\n",
            "(language (6), linguistic (26)) -> 0\n",
            "(never (7), does (55)) -> 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqD40Iq11fpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "8c30ff8d-0d5c-4d12-b67b-f5d671d06b78"
      },
      "source": [
        "#define the skip-gram model\n",
        "\n",
        "#define the model\n",
        "input_word = Input((1,))\n",
        "input_context_word = Input((1,))\n",
        "\n",
        "word_embedding = Embedding(input_dim=vocab_size, output_dim=100,input_length=1,name='word_embedding')\n",
        "context_embedding = Embedding(input_dim=vocab_size, output_dim=100,input_length=1,name='conotext_embedding')\n",
        "\n",
        "word_embedding = word_embedding(input_word)\n",
        "word_embedding_layer = Reshape((embed_size, 1))(word_embedding)\n",
        "\n",
        "context_embedding = context_embedding(input_context_word)\n",
        "context_embedding_layer = Reshape((embed_size, 1))(context_embedding)\n",
        "\n",
        "# now perform the dot product operation word_embedding_vec * context_embedding_vec\n",
        "dot_product = dot([word_embedding_layer, context_embedding_layer], axes=1)\n",
        "dot_product = Reshape((1,))(dot_product)\n",
        "\n",
        "###hint:the total number of possible labels/words\n",
        "###hint:activation='softmax' or 'sigmoid'\n",
        "outputLayer = Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "model = Model(inputs=[input_word, input_context_word], outputs=outputLayer)\n",
        "\n",
        "###hint:loss='categorical_crossentropy' or 'binary_crossentropy'\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# view model summary\n",
        "print(model.summary())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 1, 100)       8800        input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conotext_embedding (Embedding)  (None, 1, 100)       8800        input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_9 (Reshape)             (None, 100, 1)       0           word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape_10 (Reshape)            (None, 100, 1)       0           conotext_embedding[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dot_3 (Dot)                     (None, 1, 1)         0           reshape_9[0][0]                  \n",
            "                                                                 reshape_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_11 (Reshape)            (None, 1)            0           dot_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            2           reshape_11[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 17,602\n",
            "Trainable params: 17,602\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVpBqKfo1iSj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba5965d4-e3ce-4f84-ed72-de104cf2d7d1"
      },
      "source": [
        "#train the model\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "    loss = 0\n",
        "    for i, elem in enumerate(skip_grams):\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [pair_first_elem, pair_second_elem]\n",
        "        Y = labels\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 1 Loss: 4.85259735584259\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 2 Loss: 4.843343555927277\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 3 Loss: 4.834865927696228\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 4 Loss: 4.825725853443146\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 5 Loss: 4.81542044878006\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 6 Loss: 4.803482413291931\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 7 Loss: 4.789441525936127\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 8 Loss: 4.772817015647888\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 9 Loss: 4.753122687339783\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 10 Loss: 4.72987025976181\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 11 Loss: 4.702575325965881\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 12 Loss: 4.67076450586319\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 13 Loss: 4.6339845061302185\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 14 Loss: 4.591815650463104\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 15 Loss: 4.543881893157959\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 16 Loss: 4.489864885807037\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 17 Loss: 4.429516494274139\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 18 Loss: 4.362672209739685\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 19 Loss: 4.289259374141693\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 20 Loss: 4.209308564662933\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 21 Loss: 4.122959077358246\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 22 Loss: 4.030463516712189\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 23 Loss: 3.9321864247322083\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 24 Loss: 3.8286017775535583\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 25 Loss: 3.720284581184387\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 26 Loss: 3.6078976690769196\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 27 Loss: 3.4921754598617554\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 28 Loss: 3.3739049434661865\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 29 Loss: 3.253903955221176\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 30 Loss: 3.1329983174800873\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 31 Loss: 3.0120001137256622\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 32 Loss: 2.891686826944351\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 33 Loss: 2.77278408408165\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 34 Loss: 2.6559503376483917\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 35 Loss: 2.5417671501636505\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 36 Loss: 2.4307325780391693\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 37 Loss: 2.3232580423355103\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 38 Loss: 2.219670057296753\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 39 Loss: 2.1202137917280197\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 40 Loss: 2.0250579565763474\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 41 Loss: 1.9343037903308868\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 42 Loss: 1.84799163043499\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 43 Loss: 1.7661102861166\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 44 Loss: 1.6886048913002014\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 45 Loss: 1.6153847873210907\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 46 Loss: 1.5463315322995186\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 47 Loss: 1.4813047274947166\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 48 Loss: 1.4201478138566017\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 49 Loss: 1.3626932874321938\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 50 Loss: 1.308766432106495\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 51 Loss: 1.2581893876194954\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 52 Loss: 1.2107833921909332\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 53 Loss: 1.166371375322342\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 54 Loss: 1.1247799023985863\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 55 Loss: 1.0858395844697952\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 56 Loss: 1.0493875965476036\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 57 Loss: 1.0152672156691551\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 58 Loss: 0.9833288677036762\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 59 Loss: 0.9534303024411201\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 60 Loss: 0.9254369363188744\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 61 Loss: 0.899221234023571\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 62 Loss: 0.8746638707816601\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 63 Loss: 0.8516521863639355\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 64 Loss: 0.8300809524953365\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 65 Loss: 0.8098514303565025\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 66 Loss: 0.7908713333308697\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 67 Loss: 0.7730546370148659\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 68 Loss: 0.7563214004039764\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 69 Loss: 0.740597166121006\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 70 Loss: 0.7258124686777592\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 71 Loss: 0.7119029425084591\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 72 Loss: 0.6988087370991707\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 73 Loss: 0.6864743642508984\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 74 Loss: 0.674848273396492\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 75 Loss: 0.6638826504349709\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 76 Loss: 0.6535331308841705\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 77 Loss: 0.6437586080282927\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 78 Loss: 0.6345209181308746\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 79 Loss: 0.6257847305387259\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 80 Loss: 0.617517314851284\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 81 Loss: 0.6096882168203592\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 82 Loss: 0.6022692583501339\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 83 Loss: 0.5952342785894871\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 84 Loss: 0.5885588750243187\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 85 Loss: 0.5822206623852253\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 86 Loss: 0.5761985871940851\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 87 Loss: 0.570473238825798\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 88 Loss: 0.5650264415889978\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 89 Loss: 0.5598414074629545\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 90 Loss: 0.5549025442451239\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 91 Loss: 0.550195287913084\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 92 Loss: 0.5457060374319553\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 93 Loss: 0.5414221975952387\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 94 Loss: 0.5373319108039141\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 95 Loss: 0.5334241613745689\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 96 Loss: 0.5296887494623661\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 97 Loss: 0.5261160582304001\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 98 Loss: 0.5226971320807934\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 99 Loss: 0.5194236394017935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjhkbLsp1k0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the embeding matrix\n",
        "weights = model.get_weights()\n",
        "## Save the wordvectors\n",
        "f = open('skipgram_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = model.get_weights()[0]\n",
        "for key in vocab:\n",
        "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
        "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
        "f.close()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4rUPCJC1mvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "1c69bb5c-2485-4e37-c3c1-5de56aab90a9"
      },
      "source": [
        "## Load the vectors back and validate\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./skipgram_vectors.txt', binary=False)\n",
        "w2v.most_similar(positive=['the'])\n",
        "w2v.most_similar(negative=['the'])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('associations', 0.26163050532341003),\n",
              " ('has', 0.2538308799266815),\n",
              " ('when', 0.23669277131557465),\n",
              " ('not', 0.22886672616004944),\n",
              " ('non-random', 0.2182091474533081),\n",
              " ('word', 0.21514591574668884),\n",
              " ('of', 0.20937611162662506),\n",
              " ('how', 0.19063930213451385),\n",
              " ('be', 0.17910176515579224),\n",
              " ('results', 0.17082341015338898)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs8ng_Zf1o08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Excerise: modeify the skipegram_model to share the same embeding layer between word and context\n",
        "#Discussion: which is better? Why?  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}