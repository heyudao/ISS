{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Classifier_workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "40YIX2RMICLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install plot_keras_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OzlTSEZFOj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Jul 15 21:39:26 2020\n",
        "\n",
        "@author: isswan\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D,Reshape, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate\n",
        "from keras.models import load_model\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from plot_keras_history import plot_history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU8yZsHLIApG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Preparation\n",
        "import pandas as pd\n",
        "news=pd.read_table('drive/My Drive/Colab Data/r8-full-data.txt',header=None,names = [\"Class\", \"Text\"])\n",
        "news.head()\n",
        "a = news.groupby(\"Class\")\n",
        "a.head()\n",
        "a.describe()\n",
        "y=news['Class']\n",
        "X=news['Text']\n",
        "\n",
        "##check the length to determine the document length for DNN\n",
        "\n",
        "length=news['Text'].apply(len)\n",
        "news=news.assign(Length=length)\n",
        "\n",
        "#Plot the distribution of the document length for each category\n",
        "import matplotlib.pyplot as plt\n",
        "news.hist(column='Length',by='Class',bins=50)\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFHYtfQ4JXZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a7f6bdc1-362a-403f-db7a-8ec0e359fcde"
      },
      "source": [
        "#Data Preprocessing \n",
        "\n",
        "# Split train & test\n",
        "text_train, text_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "#tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_train)\n",
        "X_train = tokenizer.texts_to_sequences(text_train)\n",
        "X_test = tokenizer.texts_to_sequences(text_test)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "maxlen = max(len(x) for x in X_train) # longest text in train set\n",
        "print('vocabubary size:',vocab_size)\n",
        "print('max length text:',maxlen)\n",
        "\n",
        "#Padding the sentences\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#embedding dimension should align with the GLOVE\n",
        "embedding_dim = 100\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "GLOVE_6B_100D_PATH =\"drive/My Drive/Colab Data/glove.6B.100d.txt\"\n",
        "encoding=\"utf-8\"\n",
        "with open(GLOVE_6B_100D_PATH, \"rb\") as lines:\n",
        "    embeddings_index = {line.split()[0].decode(encoding): np.array(line.split()[1:],dtype=np.float32)\n",
        "               for line in lines}\n",
        "\n",
        "# Prepare embedding matrix from pre-trained model\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Check % words with embeddings \n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "print(nonzero_elements / vocab_size)   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabubary size: 19998\n",
            "max length text: 964\n",
            "0.848034803480348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHhOokCoFcPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bca05c88-c1db-4294-dfc1-f1bd06cafede"
      },
      "source": [
        "#create class weight dict for unbanlanced dataset\n",
        "#If 'balanced', class weights will be given by n_samples / (n_classes * np.bincount(y)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "class_weights_d = dict(enumerate(class_weights))\n",
        "\n",
        "##onehot encoding for y\n",
        "def categoricalList2Onehot(listOfClasses):\n",
        "    # integer encode\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(listOfClasses)\n",
        "    # binary encode\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "    \n",
        "    return onehot_encoded\n",
        "\n",
        "y_train_onehot = categoricalList2Onehot(y_train)\n",
        "y_train_onehot.shape\n",
        "y_test_onehot = categoricalList2Onehot(y_test)\n",
        "y_test_onehot.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2303, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzM_Vh2oH6L-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shallow CNN\n",
        "# Todo: replace the \"???\" with correct piece of code\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=???, output_dim=???, input_length=maxlen,weights=[embedding_matrix],trainable=False))\n",
        "model.add(Conv1D(512, 3, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(8, activation=???))\n",
        "model.compile(optimizer='adam',\n",
        "              loss=???,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8HuFgqLJug8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the model\n",
        "\n",
        "hist = model.fit(X_train, y_train_onehot,\n",
        "                    class_weight = class_weights_d,\n",
        "                    epochs=5,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, y_test_onehot),\n",
        "                    batch_size=64).history\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train, y_train_onehot, verbose=True)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test_onehot, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQVjdrrYWqY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Excersice : Modify Shallow CNN with dropout Layer\n",
        "#To do: add Dropout layer within the model defination and correct the ???\n",
        "\n",
        "model_dp = Sequential()\n",
        "model_dp.add(Embedding(input_dim=???, output_dim=???, input_length=maxlen,weights=[embedding_matrix],trainable=False))\n",
        "model_dp.add(Conv1D(512, 3, activation='relu'))\n",
        "model_dp.add(GlobalMaxPooling1D())\n",
        "model_dp.add(Dense(8, activation=???))\n",
        "model_dp.compile(optimizer=???,\n",
        "              loss=???,\n",
        "              metrics=['accuracy'])\n",
        "model_dp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6g0bn54W7mC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the model\n",
        "\n",
        "hist_dp = model_dp.fit(X_train, y_train_onehot,\n",
        "                    class_weight = class_weights_d,\n",
        "                    epochs=5,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, y_test_onehot),\n",
        "                    batch_size=64).history\n",
        "\n",
        "loss, accuracy = model_dp.evaluate(X_train, y_train_onehot, verbose=True)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model_dp.evaluate(X_test, y_test_onehot, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(hist_dp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT74T7V-XBYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Deep CNN \n",
        "# define the model\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "def define_model(length, vocab_size,embedding_dim):\n",
        "\t# channel 1\n",
        "\tinputs1 = Input(shape=(length,))\n",
        "\tembedding1 = Embedding(vocab_size, embedding_dim,weights=[embedding_matrix],trainable=False)(inputs1)\n",
        "\tconv1 = Conv1D(filters=128, kernel_size=2, activation='relu')(embedding1)\n",
        "\tdrop1 = Dropout(0.5)(conv1)\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "\tflat1 = Flatten()(pool1)\n",
        "\t# channel 2\n",
        "\tinputs2 = Input(shape=(length,))\n",
        "\tembedding2 = Embedding(vocab_size, embedding_dim,weights=[embedding_matrix],trainable=False)(inputs2)\n",
        "\tconv2 = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding2)\n",
        "\tdrop2 = Dropout(0.5)(conv2)\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "\tflat2 = Flatten()(pool2)\n",
        "\t# channel 3\n",
        "\tinputs3 = Input(shape=(length,))\n",
        "\tembedding3 = Embedding(vocab_size, embedding_dim,weights=[embedding_matrix],trainable=False)(inputs3)\n",
        "\tconv3 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding3)\n",
        "\tdrop3 = Dropout(0.5)(conv3)\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "\tflat3 = Flatten()(pool3)\n",
        "\t# merge\n",
        "\tmerged = concatenate([flat1, flat2, flat3])\n",
        "\t# interpretation\n",
        "\tdense1 = Dense(64, activation='relu')(merged)\n",
        "\toutputs = Dense(8, activation='softmax')(dense1)\n",
        "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\t# compile\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\treturn model\n",
        "\n",
        "model_deep = define_model(maxlen,vocab_size,embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQgQXiQIXCKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the model\n",
        "\n",
        "hist_deep = model_deep.fit(X_train, y_train_onehot,\n",
        "                    class_weight = class_weights_d,\n",
        "                    epochs=5,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, y_test_onehot),\n",
        "                    batch_size=64).history\n",
        "\n",
        "loss, accuracy = model_deep.evaluate(X_train, y_train_onehot, verbose=True)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model_deep.evaluate(X_test, y_test_onehot, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(hist_deep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kqYirJxXPJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compare all the models\n",
        "\n",
        "plt.plot(hist['loss'], label='shallow CNN')\n",
        "plt.plot(hist_dp['loss'], label='CNN with dropout')\n",
        "plt.plot(hist_deep['loss'], label='Deep CNN')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(hist['val_accuracy'], label='shallow CNN')\n",
        "plt.plot(hist_dp['val_accuracy'], label='CNN with dropout')\n",
        "plt.plot(hist_deep['val_accuracy'], label='Deep CNN')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}